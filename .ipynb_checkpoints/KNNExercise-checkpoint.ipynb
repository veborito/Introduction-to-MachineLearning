{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b66e55-cabb-4c12-a61c-898d8b573b9b",
   "metadata": {},
   "source": [
    "# Nearest neighbour classifier\n",
    "\n",
    "Here we have a nearest neighbour classifier.\n",
    "It obtains data points $(x_t, y_t)$, with $x_t \\in X$, $y_t \\in \\{0,1, \\ldots, m-1\\}$ and $t \\in \\{1, 2, \\ldots, T\\}$.\n",
    "\n",
    "Given a specific metric $d : X \\times X \\to \\mathbb{R}$, can calculate the distance $d(x_t, x)$ of each data point $x_t$ to a new point $x$.\n",
    "\n",
    "Note that a distance (aka metric) $d$ satisfies\n",
    "- Zero: $d(x, x) = 0$.\n",
    "- Positivity: $d(x, w) > 0, x \\neq w$.\n",
    "- Symmetry: $d(x, w) = d(w, x)$.\n",
    "- Triangle inequality: $d(x, z) \\leq d(x, w) + d(w z)$.\n",
    "\n",
    "If $t^* = \\arg\\min_t d(x_t, x)$ is the closest point to $x$, then the classifier returns its label, $y_{t^*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a55e46fa-4ddf-47d1-a0d9-b202f0d8cb04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "## The Nearest Neighbour Classifier\n",
    "## \n",
    "## This is the nearest neighbour classifier.\n",
    "## Given a set of data, and a specific metric,\n",
    "## it calculates all distances to a new point x.\n",
    "## It then uses the class of the closest point to x to predict the label of the new point.\n",
    "class NearestNeighbourClassifier:\n",
    "    ## Initialise the neighbours with a specific metric function and dataset\n",
    "    ## Assume labels are in {0, 1, ..., m - 1}\n",
    "    def __init__(self, data, labels, metric):\n",
    "        self.metric = metric\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.n_classes = len(np.unique(labels))  # Counts actual number of labels\n",
    "        self.n_points = data.shape[0]\n",
    "        self.n_features = data.shape[1]\n",
    "        print(\"Nearest Neighbour Set Up with classes: \", self.n_classes)\n",
    "        \n",
    "    \n",
    "    ## predict the most lik\n",
    "    def predict(self, x):\n",
    "        # calculate all distances using self.metric()\n",
    "        # return the y value for the closest point using np.argmin()\n",
    "        distance = np.array([self.metric(self.data[t], x) for t in range(self.n_points - 1)])\n",
    "        return self.labels[np.argmin(distance)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0efd10-e486-471a-b5a7-fe0ee9e30e01",
   "metadata": {},
   "source": [
    "# Euclidean distance\n",
    "The most common metric is the Euclidean metric\n",
    "$$d(x, y) = \\|x - y\\|_2 = \\sqrt{\\sum_i |x - y|^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e00c724d-ee61-416a-a652-61571efa34a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Return the euclidean distance between two points\n",
    "##\n",
    "def euclidean_metric(x, y):\n",
    "    ## hint: use np.linalg\n",
    "    return np.linalg.norm(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a840592-c150-45af-9ff9-ed04e688230e",
   "metadata": {},
   "source": [
    "# k-nearest neighbour classifier\n",
    "\n",
    "Here we have a k-nearest neighbour classifier.\n",
    "It obtains data points $(x_t, y_t)$, with $x_t \\in X$, $y_t \\in  Y = \\{0,1, \\ldots, m-1\\}$ and $t \\in \\{1, 2, \\ldots, T\\}$.\n",
    "\n",
    "Given a $k> 0$ and a specific metric $d : X \\times X \\to \\mathbb{R}$, can calculate the distance $d(x_t, x)$ of each data point $x_t$ to a new point $x$. It first order the points according to their distance from $x$, i.e. so that\n",
    "$$d(x_t, x) \\leq d(x_{t+1}, x)$$, with point $1$ being the closest point.\n",
    "\n",
    "It then uses only the $k$ closest points to calculate the most likely label.\n",
    "\n",
    "    get_probabilities(x) \n",
    "\n",
    "This function returns the vector $p$ of probabilities for each label. In particular, we set the probability of the i-th label to be the proportion of examples with the label $i$ in the k-nearest neighbours:\n",
    "$$p_i = \\sum_{t=1}^k y_t / k$$\n",
    "\n",
    "    predict(x)\n",
    "\n",
    "Return the label with the highest probability\n",
    "\n",
    "    decide(U, x)\n",
    "\n",
    "We are given a utility function $U : A \\times Y \\to \\mathbb{R}$, which indicates the value of taking $U(a,y)$ of taking action $a$ when the true label is $y$. In simple classification problems, each action $a$ corresponds to a label, but it can also be entirely different. The problem is, of course, that we do not know the label $y$. For that reason, we must use\n",
    "get_probabilities() to estimate the probability of different labels, and then:\n",
    "1. Calculate the expected utility of each action $E[U | a, x] = \\sum_y U(a, y) P(y | x)$.\n",
    "2. Select the action with the highest expected utility $a^* = \\arg\\max_a \\E[U | a, x]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "65fcf697-9923-4f17-8ecb-76fd9a9c8eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Skeleton code to be filled in\n",
    "##\n",
    "## First, fill in predict() for k = 1\n",
    "class KNearestNeighbourClassifier:\n",
    "    ## Initialise the neighbours with a specific metric function and dataset\n",
    "    ## Assume labels are in {1, ..., m}\n",
    "    def __init__(self, data, labels, metric, K):\n",
    "        self.metric = metric\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.n_classes = len(np.unique(labels))# Counts actual number of labels\n",
    "        self.K = K\n",
    "        self.n_points = data.shape[0]\n",
    "        self.n_features = data.shape[1]\n",
    "        print(\"classes: \", self.n_classes)\n",
    "        pass\n",
    "    \n",
    "\n",
    "    ## return a vector of probabilities, one for each label\n",
    "    ## Each component of the vector corresponds to the ratio of that same label in the set of neighbours\n",
    "    def get_probabilities(self, x):\n",
    "        # calculate distances\n",
    "        # sort data using argsort\n",
    "        # get K closest neighbours\n",
    "        # get the proportion of each label\n",
    "        distance = np.array([self.metric(self.data[t], x) for t in range(self.n_points - 1)])\n",
    "        sorted_distance = np.argsort(distance)\n",
    "        pr = np.zeros(self.n_classes)\n",
    "        for i in sorted_distance:\n",
    "            pr[self.labels[i]] += 1\n",
    "        for i in pr:\n",
    "            pr /= self.K\n",
    "        return pr\n",
    "    ## predict the most likely label\n",
    "    def predict(self, x):\n",
    "        # calculate the probabilities of different classes\n",
    "        # return the y value for most likely label\n",
    "        probas = self.get_probabilities(x)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    # Gives a utility for every possible choice made by the algorithm\n",
    "    def decide(self, U, x):\n",
    "        \"\"\"\n",
    "        A method that return the action that maximise the expected utility.\n",
    "        :param U: is a 2 denominational array that indicated the utility of each action based on y.\n",
    "                    example: U = np.array([ [ 1 , -1000],\n",
    "                                            [ -1 ,    0]  ])\n",
    "                            so the U[1,0] indicated the utility of tanking the action a=1 based on y=0.\n",
    "        :param x: the test point.\n",
    "        :return: the action that maximises the expected utility max_a E[U|a,x].\n",
    "                 where E[U|a,x] = sum_y P(y|x) U(a,y).\n",
    "        \"\"\"\n",
    "        n_actions = U.shape[0]\n",
    "        n_labels = U.shape[1]\n",
    "        assert (n_labels == self.n_classes)\n",
    "        # HINT:\n",
    "        # Need to use the get_probabilities function to return the action with the highest\n",
    "        # expected utility\n",
    "        # i.e. maximising sum_y P(y|x) U(a,y)\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "98f72638-196c-4941-abff-1dc26ca16304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "69a54495-2e88-47b7-9488-aa7d39d3fb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./data/class.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f740f735-1674-45d9-aeea-a243760a2aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data[[\"Weight\", \"Height\"]].to_numpy()\n",
    "y = (data[\"Gender\"]==\"F\").to_numpy()*1 # convert True false value to 1 and 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be908dd4-c261-44f0-8259-3a80d16a52ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbour Set Up with classes:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = NearestNeighbourClassifier(X, y, euclidean_metric)\n",
    "x = [20, 160]\n",
    "classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "709b6b0f-7786-4c3e-89f0-bf3c77eb844c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNearestNeighbourClassifier(X, y, euclidean_metric, 5)\n",
    "classifier.predict([70, 170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e29896-e802-4bd6-a8f3-833e0986564d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aee9fd-84cc-44ac-9a79-cb2a3bff09a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0aad0a-fde9-41e0-8ad5-4cf8f85719b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
